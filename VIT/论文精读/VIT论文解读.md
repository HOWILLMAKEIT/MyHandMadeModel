paper with code :https://paperswithcode.com/ å¯ä»¥æŸ¥çœ‹å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å’ŒæŸä¸€æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä¼˜ç§€çš„æ¨¡å‹
è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/2010.11929/1000
ææ²è¯»è®ºæ–‡çš„é¡ºåºï¼šæ ‡é¢˜-æ‘˜è¦-å¼•è¨€-ç»“è®º-ç›¸å…³å·¥ä½œ-æ¨¡å‹-å®éªŒ-è¯„è®º

# æ ‡é¢˜ & æ‘˜è¦
>an image is worth 16X16 words
- ä¸€å¼ å›¾ç‰‡ç­‰äº16ä¹˜16çš„å•è¯
- æ„æ€æ˜¯VITå°†å›¾ç‰‡åˆ†å‰²ä¸º16X16åƒç´ çš„patches

> Invision, attention is either applied in conjunction with convolutional networks, orused to replace certain components of convolutional networks while keeping theiroverall structure in place
- ä¼ ç»Ÿçš„å›¾åƒè¯†åˆ«æ–¹æ³•è¿‡åˆ†ä¾èµ–CNN

> When pre-trained on large amounts ofdata and transferred to multiple mid-sized or small image recognition benchmarks(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while ...
- åœ¨å¤§æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒå†è¿ç§»åˆ°ä¸­å°å‹æ•°æ®é›†ä¸Šæ—¶ï¼ŒVITçš„iæ•ˆæœéå¸¸å¥½ï¼ˆå› æ­¤ä¸å†éœ€è¦å·ç§¯ï¼‰

# å¼•è¨€INTRODUCTION
>The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset
- (NLP)ç›®å‰æµè¡Œçš„æ–¹å¼æ˜¯åœ¨å¤§æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ

ä¸ºä»€ä¹ˆtransformeråº”ç”¨åœ¨å›¾åƒä¸Šæœ‰å›°éš¾ï¼Ÿ
- å¯¹NLPï¼Œå¦‚æœå¥å­é•¿åº¦ä¸ºnï¼Œè®¡ç®—çš„å¤æ‚åº¦ä¸ºO($n^2$)
- å¯¹äºå›¾ç‰‡ï¼Œä¾‹å¦‚224X224ï¼Œå¦‚æœå°†å®ƒå±•å¼€ï¼ŒåƒNLPä¸€æ ·è¿›è¡Œæ“ä½œï¼Œé‚£ä¹ˆè®¡ç®—çš„å¤æ‚åº¦ä¸ºO($224^4$)

>Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications.
- å°½é‡ä½¿ç”¨åŸç”ŸTransformerï¼Œä¸è¿›è¡Œè¿‡åº¦ä¿®æ”¹

>we split an imageinto patches and provide the sequence of linear embeddings of these patches as an input to a Trans former.
- å°†å›¾ç‰‡ï¼ˆ224X224) åˆ†å‰²ä¸ºpatchesï¼Œæ¯ä¸ªpatch(16X16)å±•å¼€ä½œä¸ºä¸€ä¸ªå…ƒç´ ã€‚è¿™æ ·å°±è§£å†³äº†å›¾ç‰‡åºåˆ—è¿‡é•¿çš„é—®é¢˜

>We train the model on image classification in supervised fashion.
- åœ¨NLPä¸­é€šå¸¸é‡‡ç”¨æ— ç›‘ç£è®­ç»ƒï¼ŒVITé‡‡ç”¨æœ‰ç›‘ç£è®­ç»ƒ

åé¢çš„ä¸¤æ®µæŒ‡å‡ºï¼ŒVIT**åœ¨ä¸­å°å‹æ•°æ®é›†ï¼ˆimageNetï¼‰è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç²¾åº¦ç•¥å°äºCNNsã€‚**
- ä½œè€…æŒ‡å‡ºåŸå› æ˜¯transformerç¼ºå°‘å½’çº³åç½®(inductive biases)
- å½’çº³åç½®æŒ‡çš„æ˜¯ï¼š1.ç›¸é‚»çš„å…ƒç´ æœ‰æ›´å¼ºçš„ç›¸å…³æ€§ã€‚2.å¹³ç§»ä¸å˜æ€§ (å…ˆç§»åŠ¨/å…ˆå·ç§¯çš„ç»“æœä¸å˜)
ç„¶è€Œï¼Œif the models are trained on larger datasets (14M-300M images)ï¼Œå³**ç”¨å¤§è§„æ¨¡æ•°æ®è¿›è¡Œä¸è®­ç»ƒåï¼ŒVITçš„ç²¾åº¦éå¸¸é«˜ã€‚**

# CONCLUSION
æ€»ç»“æœ¬é¡¹ç›®å·¥ä½œï¼š
**é‡‡ç”¨æ ‡å‡†transformeræ¨¡å‹ï¼Œå°†å›¾ç‰‡è¿›è¡Œç®€å•çš„é¢„å¤„ç†ï¼Œç”¨NLPæ–¹æ³•ç›´æ¥å¤„ç†CVé—®é¢˜**

>we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP.
- å°†å›¾ç‰‡ç†è§£ä¸ºpatchesåºåˆ—ï¼Œç±»æ¯”NLP

>While these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation.
- æœ¬é¡¹ç›®åœ¨å›¾ç‰‡åˆ†ç±»ä¸Šåº”ç”¨æ•ˆæœå¾ˆå¥½ï¼Œä¸‹ä¸€æ­¥éœ€è¦ç ”ç©¶VITåœ¨å›¾åƒåˆ†å‰²å’Œæ£€æµ‹æ–¹é¢çš„åº”ç”¨

>Another challenge is to continue exploring self supervised pre-training methods. Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre training.
- éœ€è¦åœ¨CVé¢†åŸŸç ”ç©¶å­ç›‘ç£é¢„è®­ç»ƒï¼ˆæ¨¡ä»¿NLPï¼‰ï¼ˆç›®å‰çš„CVé€šå¸¸éƒ½æ˜¯é‡‡ç”¨æœ‰ç›‘ç£çš„è®­ç»ƒï¼‰

# METHOD 

![image-20250316095202542](VITè®ºæ–‡è§£è¯».assets/image-20250316095202542.png)

## å…¨è¿‡ç¨‹çš„å…¬å¼è¡¨è¾¾ï¼š
![image-20250316095212291](VITè®ºæ–‡è§£è¯».assets/image-20250316095212291.png)
å…¶ä¸­EæŒ‡çš„æ˜¯å¾—åˆ°çš„é‚£ä¸ª196X768çš„çŸ©é˜µè¿‡äº†å…¨è¿æ¥å±‚(768X768)çš„ç»“æœ

## embedding
 å°†å›¾ç‰‡åˆ†å‰²ä¸ºpatchesï¼Œç„¶åå°†æ¯ä¸ªpatchå±•å¹³ï¼Œå¹¶åŠ å…¥ä½ç½®ç¼–ç ï¼ˆä¿è¯æœ‰ä½ç½®ä¿¡æ¯ï¼‰ï¼Œè¿™æ ·å°±å¾—åˆ°äº†ä¸€ç»„tokenï¼Œéšåå°†è¿™ä¹ˆå¤štokenè¾“å…¥åˆ°transformer encoderé‡Œé¢

 ä»¥ä¸€å¼ å›¾ç‰‡ä¸¾ä¾‹ï¼Œ
 - åŸå§‹å¤§å°å¤§å°æ˜¯224 224 3
 - åˆ†å‰²åå¾—åˆ°196ä¸ª16X16X3çš„patch,å¹¶å±•å¹³ï¼Œå³196X768
 - patch embeddding: **ä¹˜**ä»¥ä¸€ä¸ª 768X768çš„å¯å­¦ä¹ çŸ©é˜µï¼ˆå…¨è¿æ¥ï¼‰ï¼ˆ**çº¿æ€§æŠ•å½±ï¼‰**
 - **æ‹¼æ¥**ä¸Šä¸€ä¸ª**ä¸“é—¨ç”¨äºåˆ†ç±»çš„class token**ï¼ˆ1X768)ï¼Œå¾—åˆ°çš„tokenå½¢çŠ¶ä¸º 197 X 768 
 - Position embeddingï¼š **åŠ **ä¸Š197Ã—768çš„å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼Œæœ€ç»ˆè¾“å…¥Transformerçš„tokenå½¢çŠ¶ä¸º197Ã—768

## Multi-Head Attention
VIT base ä¸­é‡‡ç”¨12ä¸ªå¤´è¿›è¡Œ**å¤šå¤´è‡ªæ³¨æ„åŠ›**è®¡ç®—ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå¤´ç»´æŠ¤**å•ç‹¬çš„ Q/K/V æƒé‡çŸ©é˜µ**
12ä¸ªå¤´éƒ½å°†768ç»´æ˜ å°„åˆ°64ç»´ï¼ˆä¹Ÿå°±æ˜¯è¯´æ¯ä¸ªå¤´çš„KQVçŸ©é˜µçš„å¤§å°ä¸º768X64)
197 X 768çš„tokenè¢«åˆ†ä¸º **12ç»„ 197X64çš„K Q V** 
æ¯ä¸€ç»„çš„KQVè¿›è¡Œè‡ªæ³¨æ„åŠ›è¿ç®—åï¼Œå†æŠŠç»“æœæ‹¼èµ·æ¥ï¼Œå¾—åˆ°çš„ç»“æœä»ç„¶ä¸º197X768
	ä¿æŒç»´åº¦ä¸å˜ï¼Œè¿™æ ·åœ¨ç†è®ºä¸Šæ‰ä¸ä¼šä¸¢å¤±ä¿¡æ¯é‡
	
æˆ‘ä¸ºæ•´ä¸ªè¿‡ç¨‹ç»˜åˆ¶äº†ç®€æ˜“çš„å›¾ç‰‡ï¼š
![image-20250316095222442](VITè®ºæ–‡è§£è¯».assets/image-20250316095222442.png)

## MLP 
ä¼šå¯¹ç»´åº¦è¿›è¡Œæ”¾å¤§ï¼Œæ”¾å¤§å››å€å†æŠ•å°„å›197X768

## å¦‚ä½•è¿›è¡Œé¢„æµ‹
>Similar to BERTâ€™s [class] token, we prepend a learnable embedding to the sequence of embed
-ded patches (z 0 0 = xclass), whose state at the output of the Transformer encoder (z 0 L ) serves as theimage representation y
- å°†class tokenä½ç½®çš„è¾“å‡ºä½œä¸ºæ•´ä¸ªå›¾åƒçš„è¡¨ç¤ºå‘é‡y ï¼ˆå½¢çŠ¶ä¸º 1X768)

>a classification head is attached to z 0 L . The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.
- æœ€ç»ˆå°†è¡¨ç¤ºå‘é‡yæ˜ å°„åˆ°ç±»åˆ«ç©ºé—´å°±å¥½ï¼ˆä¾‹å¦‚å…±10ä¸ªç±»åˆ«å°±æ˜ å°„åˆ°10ä½ï¼‰
- é¢„è®­ç»ƒé˜¶æ®µç”¨MLP
- å¾®è°ƒé˜¶æ®µç›´æ¥ç”±yæ˜ å°„åˆ°åˆ†ç±»ç±»åˆ«

## å…³äºembeddingsæ–¹å¼
>Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings
- é‡‡ç”¨çš„æ˜¯1Dç¼–ç 
- 1Dç¼–ç ï¼šç”Ÿæˆå¯å­¦ä¹ çš„197Ã—768ç¼–ç çŸ©é˜µï¼ˆ197ä¸ªä½ç½®ï¼Œæ¯ä¸ªä½ç½®768ç»´ï¼‰ï¼Œç›´æ¥ä¸patch embeddingsç›¸åŠ 
- 2Dç¼–ç ï¼šåˆ†åˆ«ç”Ÿæˆè¡Œã€åˆ—ç¼–ç çŸ©é˜µï¼ˆå¦‚14Ã—768çš„è¡Œç¼–ç  + 14Ã—768çš„åˆ—ç¼–ç ï¼‰ï¼Œé€šè¿‡ç›¸åŠ æˆ–æ‹¼æ¥å¾—åˆ°æœ€ç»ˆä½ç½®ç¼–ç 
- æ–‡ç« å¯¹ä¸¤ä¸ªç¼–ç æ–¹å¼è¿›è¡Œäº†å¯¹æ¯”ï¼Œå‘ç°åŒºåˆ«ä¸å¤§
	![image-20250316095231316](VITè®ºæ–‡è§£è¯».assets/image-20250316095231316.png)

## å…³äºCLS å’Œ å…¨å±€å¹³å‡æ± åŒ–
ä¸ºäº†å¾—åˆ°ä¸€ä¸ªå›¾åƒçš„åˆ†ç±»ç‰¹å¾ï¼Œä½œè€…æŒ‡å‡ºåŠ å…¥CLSå’Œä½¿ç”¨å…¨å‰§å¹³å‡æ± åŒ–ï¼ˆGAPï¼‰éƒ½å¯ä»¥
ä½†æ˜¯ä½œè€…ç§‰æŒå‡å°å¯¹transformeræ¨¡å‹æ”¹åŠ¨çš„åŸåˆ™ï¼Œå…¨å±€å‡é‡‡ç”¨CLSåšå®éªŒ
![image-20250316095246545](VITè®ºæ–‡è§£è¯».assets/image-20250316095246545.png)

## å…³äºå¾®è°ƒ
>When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. Note that this resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.
- é€šå¸¸å¦‚æœé‡‡ç”¨æ›´å¤§å°ºåº¦çš„å›¾ç‰‡è¿›è¡Œå¾®è°ƒæ•ˆæœä¼šæ›´å¥½
- ä½†æ˜¯å¦‚æœå°ºåº¦å˜å¤§ï¼Œé‡‡ç”¨ç›¸åŒçš„patch sizeï¼ˆå¦‚16X16ï¼‰ï¼Œé‚£ä¹ˆpatchçš„æ•°é‡ä¹Ÿä¼šå¢åŠ ï¼Œè¿™å°±å¯¼è‡´äº†åŸæ¥è®­ç»ƒçš„ä½ç½®ç¼–ç å¯èƒ½æ— æ•ˆ
- æ‰€ä»¥VITé‡‡ç”¨äº†2D interpolationçš„æ–¹æ³•å¯¹ä¸é¢„è®­ç»ƒçš„ä½ç½®ç¼–ç è¿›è¡Œå¤„ç†ï¼Œè¿›è€Œèƒ½å¤Ÿè¿›è¡Œå¾®è°ƒ

# å®éªŒ
ç”¨åˆ°çš„æ¨¡å‹ï¼š
å…¶ä¸­Dä»£è¡¨transformerè¾“å…¥çš„ä¸€ä¸ªå‘é‡çš„é•¿åº¦
![image-20250316095253686](VITè®ºæ–‡è§£è¯».assets/image-20250316095253686.png)

>ViT-L/16 means the â€œLargeâ€ variant with 16Ã—16 input patch size
- æ¨¡å‹å‘½åçš„æ–¹å¼åŒ…æ‹¬æ¨¡å‹å¤§å°å’Œpatch size
- 16è¡¨ç¤ºçš„æ˜¯ä¸€ä¸ªpatchçš„å¤§å°ä¸º16X16ï¼Œå³ä¸€ä¸ªpatchçš„å±•å¹³å¤§å°ä¸º768,é‚£ä¹ˆåœ¨patch embedding é˜¶æ®µçš„çº¿æ€§æŠ•å½±å°±åº”è¯¥æ˜¯ä¹˜ä»¥ä¸€ä¸ª768X1024çš„å…¨è¿æ¥å±‚ã€
- patch_sizeè¶Šå°ï¼Œè®¡ç®—è¶Šè´µï¼Œå› ä¸ºæ¯ä¸€ä¸ªpatchéƒ½è¦æ˜ å°„åˆ°1024ç»´ï¼Œä½†æ˜¯patchçš„æ•°é‡å¢åŠ äº†

## å®éªŒç»“æœï¼š
![image-20250316095302521](VITè®ºæ–‡è§£è¯».assets/image-20250316095302521.png)

## å®éªŒåˆ†æ
- ![image-20250316095307878](VITè®ºæ–‡è§£è¯».assets/image-20250316095307878.png)å›¾ç‰‡ä¸­ç°è‰²ä»£è¡¨å„ç§å¤§å°çš„ResNetï¼Œå¯ä»¥ç†è§£ä¸ºå·ç§¯ç½‘ç»œçš„ä¸Šä¸‹é™
- å¯ä»¥çœ‹åˆ°æ•°æ®é‡ä½çš„æƒ…å†µä¸‹ï¼ŒVITè¿œä¸å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼Œéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œè¡¨ç°è¶Šæ¥è¶Šå¥½ã€‚
- å¦‚æœä½ æƒ³ç”¨VITï¼Œä½ è‡³å°‘è¦ImageNet-21kè¿™ä¹ˆå¤§çš„æ•°æ®é›†

![image-20250316095320400](VITè®ºæ–‡è§£è¯».assets/image-20250316095320400.png)
- åœ¨ç›¸åŒè®¡ç®—é‡çš„æƒ…å†µä¸‹ï¼ŒVITçš„æ•ˆæœæ¯”ResNetå¥½ï¼Œè¿™è¡¨æ˜VITæ›´ä¾¿å®œ
- åœ¨å°æ•°æ®é‡çš„æƒ…å†µä¸‹ï¼Œæ··åˆæ¨¡å‹çš„æ•ˆæœå¾ˆå¥½

![image-20250316095325871](VITè®ºæ–‡è§£è¯».assets/image-20250316095325871.png)
- Eï¼ˆå°±æ˜¯å±•å¹³å¾—åˆ°çš„çŸ©é˜µç»è¿‡çº¿æ€§æŠ•å½±å¾—åˆ°çš„ç»“æœï¼‰æ˜¾ç¤ºå…¶å¯ä»¥æè¿°æ¯ä¸ªå›¾åƒå—çš„åº•å±‚ç»“æ„ğŸ¤”
- ä½ç½®ç¼–ç ç¡®å®å¯ä»¥è¡¨ç¤ºè·ç¦»ä¿¡æ¯ï¼Œä¹Ÿå­¦ä¹ åˆ°äº†è¡Œåˆ—è§„åˆ™ğŸ¤”
- éšç€ç½‘ç»œçš„åŠ æ·±ï¼Œè‡ªæ³¨æ„åŠ›è·ç¦»è¶Šæ¥è¶Šè¿œï¼Œè¡¨æ˜å¯¹å…¨å±€ä¿¡æ¯çš„å­¦ä¹ æ•ˆæœå¾ˆå¥½


# è‡ªç›‘ç£è®­ç»ƒå°è¯•
>We also perform a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT. With self-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a significant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.
- ä»¿ç…§Bertçš„å½¢å¼ï¼Œå°†ä¸€äº›patché®ç›–ï¼Œè®©æ¨¡å‹æ ¹æ®å…¶ä»–patché¢„æµ‹è¿™äº›patch
- ä½†æ˜¯è¿™æ ·åšç²¾åº¦å¹¶æ²¡æœ‰å¾ˆå¤§æå‡

# ä¸ªäººæ€»ç»“
å…¨æ–‡è´¯å½»ä¸€ä¸ªæ€æƒ³ï¼š**ç”¨åŸç”ŸtransformeråšCVå¤„ç†**ï¼Œç»“æœå‘ç°æ•ˆæœç¡®å®å¾ˆå¥½
è¿™æ— ç–‘å¼€äº†ä¸‡ç‰©çš†å¯transformerçš„å…ˆæ²³ï¼Œæ­¤åtransformerä»NLPæ¸—é€åˆ°CVï¼Œå®ç°äº†å¤§ä¸€ç»Ÿã€‚è¿™ä¹Ÿè¿›ä¸€æ­¥æ¨åŠ¨äº†å¤šæ¨¡æ€çš„å‘å±•ã€‚
åŒæ—¶ä¸€ç¯‡æ–‡ç« æå‡ºäº†nä¸ªæ–°çš„ç ”ç©¶é¢†åŸŸï¼Œä¾‹å¦‚æ¢ç´¢å¦‚ä½•ç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒVITï¼Œå°†VITåº”ç”¨åˆ°å›¾åƒåˆ†å‰²ã€æ£€æµ‹å·¥ä½œç­‰ç­‰ã€‚

